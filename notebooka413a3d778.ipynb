{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TP : apprentissage multimodal\n\n\nDans ce TP, nous allons utiliser le modèle d'apprentissage, FashionCLIP, pré-entraîné sur des images ainsi que des descriptions en langage naturel. Plus particulièrement, nous allons considérer deux cas d'usage :\n\n*   **Moteur de recherche d'images :** il s'agit de trouver, à partir d'une requête en langage naturel, l'image correspondante.\n\n*   **Classification zero-shot :** il s'agit simplement de construire un classifieur d'images (faire correspondre un label à une image).\n\n","metadata":{"id":"xfz7BSY7SP5P"}},{"cell_type":"markdown","source":"## Dataset\n\nNous allons dans un premier temps télécharger les données. Celles-ci provienennt de [Kaggle](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations).","metadata":{"id":"VZ3s403V5LKs"}},{"cell_type":"code","source":"%%capture\n!pip install gdown\n!gdown \"1igAuIEW_4h_51BG1o05WS0Q0-Cp17_-t&confirm=t\"\n!unzip data","metadata":{"id":"mLyWzNhJwoS2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modèle FashionCLIP\n\nNous allons également télécharger le modèle pré-entraîné.","metadata":{"id":"4dzpM2oASwM6"}},{"cell_type":"code","source":"%%capture\n!pip install -U fashion-clip","metadata":{"id":"tyBLvPLgSx5h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n#sys.path.append(\"fashion-clip/\")\nfrom fashion_clip.fashion_clip import FashionCLIP\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.linear_model import LogisticRegression","metadata":{"id":"WijCpqbIyH7T","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfclip = FashionCLIP('fashion-clip')","metadata":{"id":"-xEzYFUbydJY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FashionCLIP, à l'instar de CLIP, crée un espace vectoriel partagé pour les images et le texte. Cela permet de nombreuses applications, telles que la recherche (trouver l'image la plus similaire à une requête donnée) ou la classification zero-shot.\n\nIl y a principalement deux composants : un encodeur d'image (pour générer un vecteur à partir d'une image) et un encodeur de texte (pour générer un vecteur à partir d'un texte).\n\n\n\n\n\n\n","metadata":{"id":"ViPu0y8C0UfS"}},{"cell_type":"markdown","source":"\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*FLNMtW6jK51fm7Og\"  width=\"400\">\n\n","metadata":{"id":"8oc2jvxPpFWQ"}},{"cell_type":"markdown","source":"Nous allons télécharger les données que nous allons ensuite nettoyer.","metadata":{"id":"AsPVQgNphwFX"}},{"cell_type":"code","source":"articles = pd.read_csv(\"data_for_fashion_clip/articles.csv\")\n\n# Supprimer les éléments ayant la même description\nsubset = articles.drop_duplicates(\"detail_desc\").copy()\n\n# Supprimer les images dont la catégrie n'est pas renseignée\nsubset = subset[~subset[\"product_group_name\"].isin([\"Unknown\"])]\n\n# Garder seulement les descriptions dont la longueur est inférieure à 40 tokens\nsubset = subset[subset[\"detail_desc\"].apply(lambda x : 4 < len(str(x).split()) < 40)]\n\n# Supprimer les articles qui ne sont pas suffisamment fréquents dans le jeu de données\nmost_frequent_product_types = [k for k, v in dict(Counter(subset[\"product_type_name\"].tolist())).items() if v > 10]\nsubset = subset[subset[\"product_type_name\"].isin(most_frequent_product_types)]\n\nsubset.head(3)","metadata":{"id":"9emW_P2fhxSW","outputId":"54f24f03-095a-4967-dffa-0356509e8f26","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subset.to_csv(\"subset_data.csv\", index=False)\nf\"Il y a {len(subset)} éléments dans le dataset\"","metadata":{"id":"kTiCnV7Nko5L","outputId":"492b1053-7a82-45dc-f816-83b206e98779","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Moteur de recherche d'images\n\nConstuire un moteur de recherche qui permet, à partir d'une description en langage naturel, de récupérer l'image correspondante. Mesurer ses performances (précision).\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cnKHgLAumVyuHuK9pkqr7A.gif\"  width=\"800\">\n","metadata":{"id":"TcLNKhgD75pm"}},{"cell_type":"code","source":"images = [\"data_for_fashion_clip/\" + str(k) + \".jpg\" for k in subset[\"article_id\"].tolist()]\ntexts = subset[\"detail_desc\"].tolist()\n\n# Créer les représentations vectorielles (embeddings) des images et des descriptions.\nimage_embeddings = fclip.encode_images(images, batch_size=32)\ntext_embeddings = fclip.encode_text(texts, batch_size=32)","metadata":{"id":"cla9wews4eZg","outputId":"52a7ae35-5be7-484a-ac40-3270321fa675","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(image_embeddings.shape)\nprint(text_embeddings.shape)","metadata":{"id":"P5TKhC_NeKp3","outputId":"001f3e91-09d3-4dca-bf1e-a7af4213001c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definisson `get_image_by_text_embedding` qui renvoie les top k embeddings d'image les plus proches de l'ebedding de texte donne.","metadata":{}},{"cell_type":"code","source":"def get_image_by_text_embedding(text_embeddings, image_embeddings,images, text_idx=0, top_k=5):\n    embedded_txt = text_embeddings[text_idx]\n    similarities = []\n    for image_embedding in image_embeddings:\n        # Calcul du produit scalaire entre l'embedding de texte et l'embedding d'image\n        dot_product = np.dot(embedded_txt, image_embedding)\n        # Calcul de la norme de l'incorporation d'image\n        norm_image_embedding = np.linalg.norm(image_embedding)\n        # Calcul de la similarité entre l'embedding de texte et l'embedding d'image\n        similarity = dot_product / (norm_embedded_txt * norm_image_embedding)\n        # Ajoute la similarité à la liste\n        similarities.append(similarity)\n    # Trie les indices des images en fonction de leur similarité décroissante\n    sorted_image_indices = np.argsort(similarities)[::-1]\n    # Sélectionne les indices des images les plus similaires\n    top_k_closest_image_idxs = sorted_image_indices[:top_k]\n    return top_k_closest_image_idxs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definissons une fonction qui calcule l'accuracy du modele pour la prediction d'embeddings d'images.","metadata":{}},{"cell_type":"code","source":"def compute_get_image_accuracy(text_embeddings, image_embeddings, images):\n    correct_predictions_count = 0\n    for idx, text_embedding in enumerate(text_embeddings):\n        closest_image_idxs = get_image_by_text_embedding(text_embeddings, image_embeddings, images, text_idx=idx, top_k=5)\n        closest_images = [images[k] for k in closest_image_idxs]\n        if images[idx] in closest_images:\n            correct_predictions_count += 1\n    return correct_predictions_count / len(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcul de l'accuracy\naccuracy = compute_get_image_accuracy(text_embeddings, image_embeddings, images)\nprint(f\"Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification zero-shot\n\nConstruite un classsifieur d'images (prédire le label d'une image). Mesurer ses performances.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*No6ZONpQMIcfFaNMOI5oNw.gif\"  width=\"800\">\n\n","metadata":{"id":"87URDy7xh65d"}},{"cell_type":"markdown","source":"Definissons `find_closest_label` qui permet de recuperer le label le plus proche pur un embedding d'image donne.","metadata":{}},{"cell_type":"code","source":"def find_closest_label(image_embeddings, text_embeddings, labels, image_idx=0, top_k=5):\n    embedded_img = image_embeddings[image_idx]\n    similarities = []\n    for text_embedding in text_embeddings:\n        dot_product = np.dot(embedded_img, text_embedding)\n        norm_product = (np.linalg.norm(embedded_img) * np.linalg.norm(text_embedding))\n        similarities.append( dot_product / norm_product)\n    closest_label_idxs = np.argsort(similarities)[::-1][:top_k]\n    return [labels[i] for i in closest_label_idxs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculons ensuite l'accuracy des predictions avec la fonction `compute_label_accuracy`.","metadata":{}},{"cell_type":"code","source":"def compute_label_accuracy(image_embeddings, text_embeddings, labels):\n    correct_predicted_label = 0\n    for idx, image_embedding in enumerate(image_embeddings):\n        closest_labels = find_closest_label(image_embeddings, text_embeddings, labels, image_idx=idx, top_k=5)\n        if labels[idx] in closest_labels:\n            correct_predicted_label += 1\n    return correct_predicted_label / len(image_embeddings)\n\nlabel_accuracy = compute_label_accuracy(image_embeddings, text_embeddings, texts)\nprint(f\"Label Accuracy: {label_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}